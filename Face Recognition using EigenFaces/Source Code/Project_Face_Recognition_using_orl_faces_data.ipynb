{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"Project_Face_Recognition_using_orl_faces_data.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Wbrr_a4TZG4s","colab_type":"text"},"source":["### Face recognition with PCA\n","Now let's apply PCA to a face-recognition problem. Face recognition is the\n","supervised classification task of identifying a person from an image of his or her\n","face. In this example, we will use a data set called Our Database of Faces from AT&T\n","Laboratories, Cambridge. The data set contains ten images each of forty people.\n","The images were created under different lighting conditions, and the subjects varied\n","their facial expressions. The images are gray scale and 92 x 112 pixels in dimension.\n","The following is an example image:"]},{"cell_type":"markdown","metadata":{"id":"HiMw246iZG4z","colab_type":"text"},"source":["While these images are small, a feature vector that encodes the intensity of every\n","pixel will have 10,304 dimensions. Training from such high-dimensional data could\n","require many samples to avoid over-fitting. Instead, we will use PCA to compactly\n","represent the images in terms of a small number of principal components.\n","We can reshape the matrix of pixel intensities for an image into a vector, and\n","create a matrix of these vectors for all of the training images. Each image is a\n","linear combination of this data set's principal components. In the context of face\n","recognition, these principal components are called eigenfaces. The eigenfaces can\n","be thought of as standardized components of faces. Each face in the data set can\n","be expressed as some combination of the eigenfaces,"]},{"cell_type":"markdown","metadata":{"id":"CXrXYS8BZG43","colab_type":"text"},"source":["##### Download Link: \n","http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n","##### Preview Data:\n","http://www.cl.cam.ac.uk/research/dtg/attarchive/facesataglance.html\n","##### Download Install\n","conda config --add channels conda-forge\n","\n","conda install mahotas"]},{"cell_type":"code","metadata":{"id":"_Cy7H3rmZG45","colab_type":"code","colab":{},"outputId":"68c41ddd-47b6-4c9e-fc61-cb1ec7b1fbba"},"source":["import os\n","from os import walk,path,listdir\n","import numpy as np\n","import mahotas as mh\n","import warnings\n","#Mahotas currently has over 100 functions for image processing and computer vision \n","#Mahotas is a set of functions for image processing and computer vision in Python. \n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import scale\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","with warnings.catch_warnings():\n","        warnings.filterwarnings(\"ignore\", message='UserWarning')\n","\n","X = []\n","y = []\n","# We begin by loading the images into NumPy arrays, and reshaping their matrices into vectors:\n","for dir_path, dir_names, file_names in walk('/Users/ankitasinha/Desktop/data/orl_faces'):\n","    for dir_name in dir_names:\n","        print(dir_name)\n","        for fn in listdir(os.path.join(dir_path,dir_name)):\n","            if fn[-3:] == 'pgm':\n","                print(fn)\n","                image_filename = path.join(dir_path,dir_name, fn)\n","                X.append(scale(mh.imread(image_filename, as_grey=True).reshape(10304).astype('float32')))\n","                y.append(path.join(dir_path,dir_name))\n","X = np.array(X)\n","X=scale(X)\n","print(X.shape)\n","#print(np.array(y).shape)\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\n","pca = PCA(n_components=150)\n","\n","# We reduce all of the instances to 150 dimensions and train a logistic regression\n","# classifier. The data set contains forty classes; scikit-learn automatically creates\n","# binary classifiers using the one versus all strategy behind the scenes:\n","X_train_reduced = pca.fit_transform(X_train)\n","X_test_reduced = pca.transform(X_test)\n","print ('The original dimensions of the training data were', X_train.shape)\n","print ('The original dimensions of the test data were', X_test.shape)\n","print ('The reduced dimensions of the training data are', X_train_reduced.shape)\n","print ('The reduced dimensions of the test data were', X_test_reduced.shape)\n","\n","classifier = LogisticRegression()\n","#print(y_train)\n","accuracies = cross_val_score(classifier, X_train_reduced, y_train)\n","print(accuracies)\n","\n","# Finally, we evaluate the performance of the classifier using cross-validation and a\n","# test set. The average per-class F1 score of the classifier trained on the full data was\n","# 0.94, but required significantly more time to train and could be prohibitively slow in\n","# an application with more training instances:\n","print ('Cross validation accuracy:', np.mean(accuracies),accuracies)\n","classifier.fit(X_train_reduced, y_train)\n","predictions = classifier.predict(X_test_reduced)\n","print(classification_report(y_test, predictions))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["s1\n","1.pgm\n","10.pgm\n"],"name":"stdout"},{"output_type":"stream","text":["/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n","  warnings.warn(\"Numerical issues were encountered \"\n","/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n","  warnings.warn(\"Numerical issues were encountered \"\n"],"name":"stderr"},{"output_type":"stream","text":["2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s10\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s11\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s12\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s13\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s14\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s15\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s16\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s17\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s18\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s19\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s2\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s20\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s21\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s22\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s23\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s24\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s25\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s26\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s27\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s28\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s29\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s3\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s30\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s31\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s32\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s33\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s34\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s35\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s36\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s37\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s38\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s39\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s4\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s40\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s5\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s6\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s7\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s8\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","s9\n","1.pgm\n","10.pgm\n","2.pgm\n","3.pgm\n","4.pgm\n","5.pgm\n","6.pgm\n","7.pgm\n","8.pgm\n","9.pgm\n","(400, 10304)\n","The original dimensions of the training data were (300, 10304)\n","The original dimensions of the test data were (100, 10304)\n","The reduced dimensions of the training data are (300, 150)\n","The reduced dimensions of the test data were (100, 150)\n","[0.81981982 0.7979798  0.72222222]\n","Cross validation accuracy: 0.78000728000728 [0.81981982 0.7979798  0.72222222]\n","                                               precision    recall  f1-score   support\n","\n"," /Users/ankitasinha/Desktop/data/orl_faces/s1       0.33      1.00      0.50         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s10       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s11       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s12       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s13       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s14       1.00      1.00      1.00         2\n","/Users/ankitasinha/Desktop/data/orl_faces/s15       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s16       1.00      1.00      1.00         4\n","/Users/ankitasinha/Desktop/data/orl_faces/s17       1.00      1.00      1.00         4\n","/Users/ankitasinha/Desktop/data/orl_faces/s18       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s19       1.00      1.00      1.00         3\n"," /Users/ankitasinha/Desktop/data/orl_faces/s2       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s20       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s21       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s22       1.00      1.00      1.00         2\n","/Users/ankitasinha/Desktop/data/orl_faces/s23       1.00      1.00      1.00         2\n","/Users/ankitasinha/Desktop/data/orl_faces/s24       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s25       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s26       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s27       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s28       1.00      1.00      1.00         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s29       1.00      1.00      1.00         2\n"," /Users/ankitasinha/Desktop/data/orl_faces/s3       1.00      1.00      1.00         4\n","/Users/ankitasinha/Desktop/data/orl_faces/s30       1.00      1.00      1.00         4\n","/Users/ankitasinha/Desktop/data/orl_faces/s31       0.33      1.00      0.50         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s32       1.00      0.67      0.80         3\n","/Users/ankitasinha/Desktop/data/orl_faces/s33       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s34       1.00      0.60      0.75         5\n","/Users/ankitasinha/Desktop/data/orl_faces/s35       1.00      1.00      1.00         2\n","/Users/ankitasinha/Desktop/data/orl_faces/s36       1.00      0.80      0.89         5\n","/Users/ankitasinha/Desktop/data/orl_faces/s37       1.00      1.00      1.00         4\n","/Users/ankitasinha/Desktop/data/orl_faces/s38       1.00      1.00      1.00         2\n","/Users/ankitasinha/Desktop/data/orl_faces/s39       1.00      1.00      1.00         2\n"," /Users/ankitasinha/Desktop/data/orl_faces/s4       1.00      1.00      1.00         1\n","/Users/ankitasinha/Desktop/data/orl_faces/s40       0.50      1.00      0.67         1\n"," /Users/ankitasinha/Desktop/data/orl_faces/s5       1.00      0.67      0.80         3\n"," /Users/ankitasinha/Desktop/data/orl_faces/s6       1.00      1.00      1.00         4\n"," /Users/ankitasinha/Desktop/data/orl_faces/s7       1.00      1.00      1.00         4\n"," /Users/ankitasinha/Desktop/data/orl_faces/s8       1.00      1.00      1.00         1\n"," /Users/ankitasinha/Desktop/data/orl_faces/s9       1.00      1.00      1.00         3\n","\n","                                  avg / total       0.98      0.95      0.96       100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vCQZogsmZG5G","colab_type":"code","colab":{}},"source":["PCA?"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Imk6kHFfZG5L","colab_type":"text"},"source":["# Visualization\n","def plot_gallery(images, titles, h, w, rows=3, cols=4):\n","    plt.figure()\n","    for i in range(rows * cols):\n","        plt.subplot(rows, cols, i + 1)\n","        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n","        plt.title(titles[i])\n","        plt.xticks(())\n","        plt.yticks(())\n","\n","def titles(y_pred, y_test, target_names):\n","    for i in range(y_pred.shape[0]):\n","        pred_name = target_names[y_pred[i]].split(' ')[-1]\n","        true_name = target_names[y_test[i]].split(' ')[-1]\n","        yield 'predicted: {0}\\ntrue: {1}'.format(pred_name, true_name)\n","        \n","prediction_titles = list(titles(y_test, predictions, target_names))\n","plot_gallery(X_test, prediction_titles, h, w)"]},{"cell_type":"markdown","metadata":{"id":"LsaJUJFeZG5M","colab_type":"text"},"source":["### Conclusion -- Dimensionality reduction\n","High-dimensional data cannot be visualized easily. High-dimensional data sets may also suffer from the curse of dimensionality; estimators require many samples to learn to generalize from high-dimensional data. We mitigated these problems using a technique called principal component analysis, which reduces a\n","high-dimensional, possibly-correlated data set to a lower-dimensional set of uncorrelated principal components by projecting the data onto a lower-dimensional subspace. "]},{"cell_type":"markdown","metadata":{"id":"iucm7yYmZG5N","colab_type":"text"},"source":["Face recognition is ubiquitous in science fiction: the protagonist looks at a camera, and the camera scans his or her face to recognize the person. More formally, we can formulate face recognition as a classification task, where the inputs are images and the outputs are people’s names. We’re going to discuss a popular technique for face recognition called eigenfaces. And at the heart of eigenfaces is an unsupervised dimensionality reduction technique called principal component analysis (PCA), and we will see how we can apply this general technique to our specific task of face recognition"]},{"cell_type":"markdown","metadata":{"id":"_6d76TwhZG5P","colab_type":"text"},"source":["Face recognition is the challenge of classifying whose face is in an input image. This is different than face detection where the challenge is determining if there is a face in the input image. With face recognition, we need an existing database of faces. Given a new image of a face, we need to report the person’s name."]},{"cell_type":"markdown","metadata":{"id":"0oGi5vJfZG5R","colab_type":"text"},"source":["Princpal Component Analysis\n","One technique of dimensionality reduction is called principal component analysis (PCA). The idea behind PCA is that we want to select the hyperplane such that when all the points are projected onto it, they are maximally spread out. In other words, we want the axis of maximal variance! Let’s consider our example plot above. A potential axis is the x-axis or y-axis, but, in both cases, that’s not the best axis. However, if we pick a line that cuts through our data diagonally, that is the axis where the data would be most spread!\n"]},{"cell_type":"markdown","metadata":{"id":"pmXOfEmkZG5T","colab_type":"text"},"source":["Essentially, we compute the covariance matrix of our data and consider that covariance matrix’s largest eigenvectors. Those are our principal axes and the axes that we project our data onto to reduce dimensions. Using this approach, we can take high-dimensional data and reduce it down to a lower dimension by selecting the largest eigenvectors of the covariance matrix and projecting onto those eigenvectors."]},{"cell_type":"markdown","metadata":{"id":"fn40nsVRZG5V","colab_type":"text"},"source":["https://pythonmachinelearning.pro/face-recognition-with-eigenfaces/"]},{"cell_type":"markdown","metadata":{"id":"orYkyS0LZG5X","colab_type":"text"},"source":["The easiest way to create a dataset for face recognition is to create a folder for each person and put the face images in there. Make sure each are the same size and resize them so they aren’t large images! Remember that PCA will reduce the image’s dimensionality when we project onto that space anyways so using large, high-definition images won’t help and will slow down our algorithm. A good size is ~512×512 for each image. The images should all be the same size so you can store them in one numpy array with dimensions (num_examples, height, width) . (We’re assuming grayscale images). Then use the folder names to disambiguate classes. Using this approach, you can use your own images.\n","\n"]},{"cell_type":"code","metadata":{"id":"VZ7jPR8oZG5Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}